{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole: REINFORCE Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent <b>that plays Cartpole </b>\n",
    "\n",
    "<img src=\"http://neuro-educator.com/wp-content/uploads/2017/09/DQN.gif\" alt=\"Cartpole gif\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Policy gradients [Article](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "This time we use <a href=\"https://gym.openai.com/\">OpenAI Gym</a> which has a lot of great environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ExSidius/.local/share/virtualenvs/cmsc389f_final_project-26nFM2SL/lib/python3.5/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up our hyperparameters ‚öóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT Hyperparameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95 # Discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Define the preprocessing functions ‚öôÔ∏è\n",
    "This function takes <b>the rewards and perform discounting.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/Policy%20Gradients/Cartpole/assets/catpole.png\">\n",
    "\n",
    "The idea is simple:\n",
    "- Our state which is an array of 4 values will be used as an input.\n",
    "- Our NN is 3 fully connected layers.\n",
    "- Our output activation function is softmax that squashes the outputs to a probability distribution (for instance if we have 4, 2, 6 --> softmax --> (0.4, 0.2, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
    "                                                num_outputs = 10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create the NN\n",
    "maxReward = 0 # Keep track of maximum reward\n",
    "For episode in range(max_episodes):\n",
    "    episode + 1\n",
    "    reset environment\n",
    "    reset stores (states, actions, rewards)\n",
    "    \n",
    "    For each step:\n",
    "        Choose action a\n",
    "        Perform action a\n",
    "        Store s, a, r\n",
    "        If done:\n",
    "            Calculate sum reward\n",
    "            Calculate gamma Gt\n",
    "            Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  13.0\n",
      "Mean Reward 13.0\n",
      "Max reward so far:  13.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  43.0\n",
      "Mean Reward 28.0\n",
      "Max reward so far:  43.0\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  47.0\n",
      "Mean Reward 34.333333333333336\n",
      "Max reward so far:  47.0\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  27.0\n",
      "Mean Reward 32.5\n",
      "Max reward so far:  47.0\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  13.0\n",
      "Mean Reward 28.6\n",
      "Max reward so far:  47.0\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  77.0\n",
      "Mean Reward 36.666666666666664\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  33.0\n",
      "Mean Reward 36.142857142857146\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  48.0\n",
      "Mean Reward 37.625\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  12.0\n",
      "Mean Reward 34.77777777777778\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  29.0\n",
      "Mean Reward 34.2\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  27.0\n",
      "Mean Reward 33.54545454545455\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  42.0\n",
      "Mean Reward 34.25\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  19.0\n",
      "Mean Reward 33.07692307692308\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  20.0\n",
      "Mean Reward 32.142857142857146\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  34.0\n",
      "Mean Reward 32.266666666666666\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  35.0\n",
      "Mean Reward 32.4375\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  34.0\n",
      "Mean Reward 32.529411764705884\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  37.0\n",
      "Mean Reward 32.77777777777778\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  46.0\n",
      "Mean Reward 33.473684210526315\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  75.0\n",
      "Mean Reward 35.55\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  16.0\n",
      "Mean Reward 34.61904761904762\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  14.0\n",
      "Mean Reward 33.68181818181818\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  20.0\n",
      "Mean Reward 33.08695652173913\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  41.0\n",
      "Mean Reward 33.416666666666664\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  47.0\n",
      "Mean Reward 33.96\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  30.0\n",
      "Mean Reward 33.80769230769231\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  58.0\n",
      "Mean Reward 34.7037037037037\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  25.0\n",
      "Mean Reward 34.357142857142854\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  46.0\n",
      "Mean Reward 34.758620689655174\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  16.0\n",
      "Mean Reward 34.13333333333333\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  25.0\n",
      "Mean Reward 33.83870967741935\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  12.0\n",
      "Mean Reward 33.15625\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  11.0\n",
      "Mean Reward 32.484848484848484\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  17.0\n",
      "Mean Reward 32.029411764705884\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  30.0\n",
      "Mean Reward 31.97142857142857\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  51.0\n",
      "Mean Reward 32.5\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  44.0\n",
      "Mean Reward 32.810810810810814\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  15.0\n",
      "Mean Reward 32.3421052631579\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  62.0\n",
      "Mean Reward 33.1025641025641\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  27.0\n",
      "Mean Reward 32.95\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  39.0\n",
      "Mean Reward 33.09756097560975\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  18.0\n",
      "Mean Reward 32.73809523809524\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  34.0\n",
      "Mean Reward 32.76744186046512\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  34.0\n",
      "Mean Reward 32.79545454545455\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  28.0\n",
      "Mean Reward 32.68888888888889\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  76.0\n",
      "Mean Reward 33.630434782608695\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  22.0\n",
      "Mean Reward 33.38297872340426\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  50.0\n",
      "Mean Reward 33.729166666666664\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  17.0\n",
      "Mean Reward 33.38775510204081\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  57.0\n",
      "Mean Reward 33.86\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  36.0\n",
      "Mean Reward 33.90196078431372\n",
      "Max reward so far:  77.0\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  80.0\n",
      "Mean Reward 34.78846153846154\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  42.0\n",
      "Mean Reward 34.924528301886795\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  50.0\n",
      "Mean Reward 35.2037037037037\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  43.0\n",
      "Mean Reward 35.345454545454544\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  24.0\n",
      "Mean Reward 35.142857142857146\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  28.0\n",
      "Mean Reward 35.01754385964912\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  41.0\n",
      "Mean Reward 35.12068965517241\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  29.0\n",
      "Mean Reward 35.016949152542374\n",
      "Max reward so far:  80.0\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  256.0\n",
      "Mean Reward 38.7\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  42.0\n",
      "Mean Reward 38.75409836065574\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  29.0\n",
      "Mean Reward 38.596774193548384\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  42.0\n",
      "Mean Reward 38.65079365079365\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  42.0\n",
      "Mean Reward 38.703125\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  21.0\n",
      "Mean Reward 38.43076923076923\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  28.0\n",
      "Mean Reward 38.27272727272727\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  24.0\n",
      "Mean Reward 38.059701492537314\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  26.0\n",
      "Mean Reward 37.88235294117647\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  24.0\n",
      "Mean Reward 37.68115942028985\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  85.0\n",
      "Mean Reward 38.357142857142854\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  10.0\n",
      "Mean Reward 37.95774647887324\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  31.0\n",
      "Mean Reward 37.861111111111114\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  56.0\n",
      "Mean Reward 38.10958904109589\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  55.0\n",
      "Mean Reward 38.33783783783784\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  87.0\n",
      "Mean Reward 38.986666666666665\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  59.0\n",
      "Mean Reward 39.25\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  63.0\n",
      "Mean Reward 39.55844155844156\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  101.0\n",
      "Mean Reward 40.34615384615385\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  67.0\n",
      "Mean Reward 40.68354430379747\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  120.0\n",
      "Mean Reward 41.675\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  105.0\n",
      "Mean Reward 42.45679012345679\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  30.0\n",
      "Mean Reward 42.30487804878049\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  167.0\n",
      "Mean Reward 43.80722891566265\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  52.0\n",
      "Mean Reward 43.904761904761905\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  110.0\n",
      "Mean Reward 44.68235294117647\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  76.0\n",
      "Mean Reward 45.04651162790697\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  32.0\n",
      "Mean Reward 44.89655172413793\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  42.0\n",
      "Mean Reward 44.86363636363637\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  125.0\n",
      "Mean Reward 45.764044943820224\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  82.0\n",
      "Mean Reward 46.166666666666664\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  54.0\n",
      "Mean Reward 46.252747252747255\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  153.0\n",
      "Mean Reward 47.41304347826087\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  156.0\n",
      "Mean Reward 48.58064516129032\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  154.0\n",
      "Mean Reward 49.702127659574465\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  52.0\n",
      "Mean Reward 49.72631578947368\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  101.0\n",
      "Mean Reward 50.260416666666664\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  102.0\n",
      "Mean Reward 50.79381443298969\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  138.0\n",
      "Mean Reward 51.683673469387756\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  175.0\n",
      "Mean Reward 52.92929292929293\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  118.0\n",
      "Mean Reward 53.58\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  144.0\n",
      "Mean Reward 54.475247524752476\n",
      "Max reward so far:  256.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  168.0\n",
      "Mean Reward 55.588235294117645\n",
      "Max reward so far:  256.0\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  331.0\n",
      "Mean Reward 58.262135922330096\n",
      "Max reward so far:  331.0\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  120.0\n",
      "Mean Reward 58.85576923076923\n",
      "Max reward so far:  331.0\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  92.0\n",
      "Mean Reward 59.17142857142857\n",
      "Max reward so far:  331.0\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  214.0\n",
      "Mean Reward 60.632075471698116\n",
      "Max reward so far:  331.0\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  453.0\n",
      "Mean Reward 64.29906542056075\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  238.0\n",
      "Mean Reward 65.9074074074074\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  75.0\n",
      "Mean Reward 65.9908256880734\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  210.0\n",
      "Mean Reward 67.3\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  145.0\n",
      "Mean Reward 68.0\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  292.0\n",
      "Mean Reward 70.0\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  166.0\n",
      "Mean Reward 70.84955752212389\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  343.0\n",
      "Mean Reward 73.23684210526316\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  72.0\n",
      "Mean Reward 73.22608695652174\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  240.0\n",
      "Mean Reward 74.66379310344827\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  396.0\n",
      "Mean Reward 77.41025641025641\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  262.0\n",
      "Mean Reward 78.97457627118644\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  222.0\n",
      "Mean Reward 80.17647058823529\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  426.0\n",
      "Mean Reward 83.05833333333334\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  270.0\n",
      "Mean Reward 84.60330578512396\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  333.0\n",
      "Mean Reward 86.63934426229508\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  393.0\n",
      "Mean Reward 89.130081300813\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  267.0\n",
      "Mean Reward 90.56451612903226\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  215.0\n",
      "Mean Reward 91.56\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  388.0\n",
      "Mean Reward 93.91269841269842\n",
      "Max reward so far:  453.0\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  468.0\n",
      "Mean Reward 96.85826771653544\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  310.0\n",
      "Mean Reward 98.5234375\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  180.0\n",
      "Mean Reward 99.15503875968992\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  270.0\n",
      "Mean Reward 100.46923076923076\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  223.0\n",
      "Mean Reward 101.40458015267176\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  343.0\n",
      "Mean Reward 103.23484848484848\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  319.0\n",
      "Mean Reward 104.85714285714286\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  391.0\n",
      "Mean Reward 106.99253731343283\n",
      "Max reward so far:  468.0\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  563.0\n",
      "Mean Reward 110.37037037037037\n",
      "Max reward so far:  563.0\n",
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  439.0\n",
      "Mean Reward 112.78676470588235\n",
      "Max reward so far:  563.0\n",
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  331.0\n",
      "Mean Reward 114.37956204379562\n",
      "Max reward so far:  563.0\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  221.0\n",
      "Mean Reward 115.15217391304348\n",
      "Max reward so far:  563.0\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  783.0\n",
      "Mean Reward 119.9568345323741\n",
      "Max reward so far:  783.0\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  1421.0\n",
      "Mean Reward 129.25\n",
      "Max reward so far:  1421.0\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  788.0\n",
      "Mean Reward 133.92198581560282\n",
      "Max reward so far:  1421.0\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  754.0\n",
      "Mean Reward 138.2887323943662\n",
      "Max reward so far:  1421.0\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  1572.0\n",
      "Mean Reward 148.3146853146853\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  216.0\n",
      "Mean Reward 148.78472222222223\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  332.0\n",
      "Mean Reward 150.04827586206898\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  365.0\n",
      "Mean Reward 151.5205479452055\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  281.0\n",
      "Mean Reward 152.40136054421768\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  744.0\n",
      "Mean Reward 156.39864864864865\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  278.0\n",
      "Mean Reward 157.21476510067114\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  960.0\n",
      "Mean Reward 162.56666666666666\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  377.0\n",
      "Mean Reward 163.98675496688742\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  307.0\n",
      "Mean Reward 164.92763157894737\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  539.0\n",
      "Mean Reward 167.37254901960785\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  562.0\n",
      "Mean Reward 169.93506493506493\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  309.0\n",
      "Mean Reward 170.83225806451614\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  430.0\n",
      "Mean Reward 172.49358974358975\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  366.0\n",
      "Mean Reward 173.72611464968153\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  1024.0\n",
      "Mean Reward 179.10759493670886\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  681.0\n",
      "Mean Reward 182.26415094339623\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  504.0\n",
      "Mean Reward 184.275\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  751.0\n",
      "Mean Reward 187.7950310559006\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  346.0\n",
      "Mean Reward 188.7716049382716\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  726.0\n",
      "Mean Reward 192.06748466257667\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  574.0\n",
      "Mean Reward 194.39634146341464\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  607.0\n",
      "Mean Reward 196.8969696969697\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  721.0\n",
      "Mean Reward 200.05421686746988\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  348.0\n",
      "Mean Reward 200.94011976047904\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  646.0\n",
      "Mean Reward 203.58928571428572\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  443.0\n",
      "Mean Reward 205.0059171597633\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  546.0\n",
      "Mean Reward 207.01176470588234\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  142.0\n",
      "Mean Reward 206.6315789473684\n",
      "Max reward so far:  1572.0\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  1960.0\n",
      "Mean Reward 216.82558139534885\n",
      "Max reward so far:  1960.0\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  2705.0\n",
      "Mean Reward 231.20809248554914\n",
      "Max reward so far:  2705.0\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  12641.0\n",
      "Mean Reward 302.5287356321839\n",
      "Max reward so far:  12641.0\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  13207.0\n",
      "Mean Reward 376.2685714285714\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  612.0\n",
      "Mean Reward 377.60795454545456\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  2494.0\n",
      "Mean Reward 389.56497175141243\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  1184.0\n",
      "Mean Reward 394.0280898876405\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  292.0\n",
      "Mean Reward 393.4581005586592\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  1276.0\n",
      "Mean Reward 398.3611111111111\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  1123.0\n",
      "Mean Reward 402.3646408839779\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  379.0\n",
      "Mean Reward 402.2362637362637\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  455.0\n",
      "Mean Reward 402.5245901639344\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  621.0\n",
      "Mean Reward 403.7119565217391\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  748.0\n",
      "Mean Reward 405.572972972973\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  838.0\n",
      "Mean Reward 407.89784946236557\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  218.0\n",
      "Mean Reward 406.88235294117646\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  511.0\n",
      "Mean Reward 407.43617021276594\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  682.0\n",
      "Mean Reward 408.8888888888889\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  292.0\n",
      "Mean Reward 408.2736842105263\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  348.0\n",
      "Mean Reward 407.9581151832461\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  2272.0\n",
      "Mean Reward 417.6666666666667\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  754.0\n",
      "Mean Reward 419.40932642487047\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  1742.0\n",
      "Mean Reward 426.22680412371136\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  1147.0\n",
      "Mean Reward 429.9230769230769\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  521.0\n",
      "Mean Reward 430.38775510204084\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  693.0\n",
      "Mean Reward 431.7208121827411\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  617.0\n",
      "Mean Reward 432.65656565656565\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  5660.0\n",
      "Mean Reward 458.9246231155779\n",
      "Max reward so far:  13207.0\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  13350.0\n",
      "Mean Reward 523.38\n",
      "Max reward so far:  13350.0\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  7368.0\n",
      "Mean Reward 557.4328358208955\n",
      "Max reward so far:  13350.0\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  17576.0\n",
      "Mean Reward 641.6831683168317\n",
      "Max reward so far:  17576.0\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  9177.0\n",
      "Mean Reward 683.7290640394089\n",
      "Max reward so far:  17576.0\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  15532.0\n",
      "Mean Reward 756.5147058823529\n",
      "Max reward so far:  17576.0\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  8128.0\n",
      "Mean Reward 792.4731707317073\n",
      "Max reward so far:  17576.0\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  16533.0\n",
      "Mean Reward 868.8834951456311\n",
      "Max reward so far:  17576.0\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  23378.0\n",
      "Mean Reward 977.6231884057971\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  11319.0\n",
      "Mean Reward 1027.3413461538462\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  18144.0\n",
      "Mean Reward 1109.2392344497607\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  3630.0\n",
      "Mean Reward 1121.2428571428572\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  1794.0\n",
      "Mean Reward 1124.431279620853\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  1534.0\n",
      "Mean Reward 1126.3632075471698\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  1569.0\n",
      "Mean Reward 1128.4413145539907\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  2618.0\n",
      "Mean Reward 1135.4018691588785\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  327.0\n",
      "Mean Reward 1131.6418604651162\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  260.0\n",
      "Mean Reward 1127.6064814814815\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  524.0\n",
      "Mean Reward 1124.8248847926268\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  1146.0\n",
      "Mean Reward 1124.9220183486239\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  435.0\n",
      "Mean Reward 1121.7716894977168\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  272.0\n",
      "Mean Reward 1117.909090909091\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  488.0\n",
      "Mean Reward 1115.0588235294117\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  249.0\n",
      "Mean Reward 1111.1576576576576\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  571.0\n",
      "Mean Reward 1108.7354260089687\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  227.0\n",
      "Mean Reward 1104.799107142857\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  221.0\n",
      "Mean Reward 1100.871111111111\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  299.0\n",
      "Mean Reward 1097.3230088495575\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  221.0\n",
      "Mean Reward 1093.4625550660794\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  201.0\n",
      "Mean Reward 1089.548245614035\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  226.0\n",
      "Mean Reward 1085.7772925764193\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  256.0\n",
      "Mean Reward 1082.1695652173912\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  257.0\n",
      "Mean Reward 1078.5974025974026\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  260.0\n",
      "Mean Reward 1075.0689655172414\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  289.0\n",
      "Mean Reward 1071.6952789699571\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  395.0\n",
      "Mean Reward 1068.8034188034187\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  258.0\n",
      "Mean Reward 1065.3531914893617\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  772.0\n",
      "Mean Reward 1064.1101694915253\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  310.0\n",
      "Mean Reward 1060.928270042194\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  313.0\n",
      "Mean Reward 1057.7857142857142\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  268.0\n",
      "Mean Reward 1054.4811715481171\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  212.0\n",
      "Mean Reward 1050.9708333333333\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  262.0\n",
      "Mean Reward 1047.6970954356846\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  295.0\n",
      "Mean Reward 1044.5867768595042\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  288.0\n",
      "Mean Reward 1041.4732510288065\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  305.0\n",
      "Mean Reward 1038.454918032787\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  240.0\n",
      "Mean Reward 1035.1959183673468\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  265.0\n",
      "Mean Reward 1032.0650406504064\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  438.0\n",
      "Mean Reward 1029.65991902834\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  289.0\n",
      "Mean Reward 1026.6733870967741\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  259.0\n",
      "Mean Reward 1023.5903614457832\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  342.0\n",
      "Mean Reward 1020.864\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  292.0\n",
      "Mean Reward 1017.9601593625498\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  289.0\n",
      "Mean Reward 1015.0674603174604\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  240.0\n",
      "Mean Reward 1012.00395256917\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  283.0\n",
      "Mean Reward 1009.1338582677165\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  357.0\n",
      "Mean Reward 1006.5764705882353\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  365.0\n",
      "Mean Reward 1004.0703125\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  304.0\n",
      "Mean Reward 1001.3463035019455\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  253.0\n",
      "Mean Reward 998.4457364341085\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  295.0\n",
      "Mean Reward 995.7297297297297\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  471.0\n",
      "Mean Reward 993.7115384615385\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  342.0\n",
      "Mean Reward 991.2145593869732\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  439.0\n",
      "Mean Reward 989.1068702290077\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  287.0\n",
      "Mean Reward 986.4372623574144\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  345.0\n",
      "Mean Reward 984.0075757575758\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  352.0\n",
      "Mean Reward 981.622641509434\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  419.0\n",
      "Mean Reward 979.5075187969925\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  360.0\n",
      "Mean Reward 977.187265917603\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  480.0\n",
      "Mean Reward 975.3320895522388\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  588.0\n",
      "Mean Reward 973.8921933085502\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  275.0\n",
      "Mean Reward 971.3037037037037\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  290.0\n",
      "Mean Reward 968.789667896679\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  409.0\n",
      "Mean Reward 966.7316176470588\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  417.0\n",
      "Mean Reward 964.7179487179487\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  336.0\n",
      "Mean Reward 962.4233576642335\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  319.0\n",
      "Mean Reward 960.0836363636364\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  303.0\n",
      "Mean Reward 957.7028985507246\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  312.0\n",
      "Mean Reward 955.3718411552346\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  210.0\n",
      "Mean Reward 952.6906474820144\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  266.0\n",
      "Mean Reward 950.2293906810036\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  242.0\n",
      "Mean Reward 947.7\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  232.0\n",
      "Mean Reward 945.153024911032\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  245.0\n",
      "Mean Reward 942.6702127659574\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  272.0\n",
      "Mean Reward 940.3003533568905\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  307.0\n",
      "Mean Reward 938.0704225352113\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  281.0\n",
      "Mean Reward 935.7649122807018\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  198.0\n",
      "Mean Reward 933.1853146853147\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  226.0\n",
      "Mean Reward 930.7212543554007\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  191.0\n",
      "Mean Reward 928.1527777777778\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  259.0\n",
      "Mean Reward 925.8373702422145\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  255.0\n",
      "Mean Reward 923.5241379310345\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  198.0\n",
      "Mean Reward 921.0309278350516\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  233.0\n",
      "Mean Reward 918.6746575342465\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  285.0\n",
      "Mean Reward 916.5119453924915\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  264.0\n",
      "Mean Reward 914.2925170068028\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  308.0\n",
      "Mean Reward 912.2372881355932\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  480.0\n",
      "Mean Reward 910.777027027027\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  390.0\n",
      "Mean Reward 909.023569023569\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  347.0\n",
      "Mean Reward 907.1375838926175\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  443.0\n",
      "Mean Reward 905.5852842809364\n",
      "Max reward so far:  23378.0\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  445.0\n",
      "Mean Reward 904.05\n",
      "Max reward so far:  23378.0\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        \n",
    "        env.render()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                                                                })\n",
    "                \n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                                    mean_reward_: mean_reward\n",
    "                                                                })\n",
    "                \n",
    "               \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "Score 250.0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Score 286.0\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Score 204.0\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Score 191.0\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Score 363.0\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Score 216.0\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Score 205.0\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Score 271.0\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Score 175.0\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Score 128.0\n",
      "Score over time: 228.9\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
    "            #print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
